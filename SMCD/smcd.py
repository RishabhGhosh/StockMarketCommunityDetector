# -*- coding: utf-8 -*-
"""smcd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18V4jT1JStixoJU5nfvv2bqH_87Qx5xSH
"""

# Mount point in google drive, comment it out if runner in other platform
import os
from google.colab import drive
MOUNTPOINT = '/content/gdrive'
DATADIR = os.path.join(MOUNTPOINT, 'My Drive', 'SMCD')
drive.mount(MOUNTPOINT)

import sys
sys.path.insert(0,'/content/gdrive/My Drive/SMCD')

!pip install graphviz
!pip install networkx
!pip install torch-geometric
!pip install torch torchvision
!pip install torch_xla
!pip install transformers

# Only run this if you want TPU
# !pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl

# Mention all the imports
from   collections import defaultdict
from   graphviz import Digraph
import matplotlib.pyplot as plt
import networkx as nx
import nltk
from   nltk import ngrams
from   nltk.corpus import stopwords
import numpy as np
import os
import pandas as pd
import re
from   sklearn.feature_extraction.text import CountVectorizer
import seaborn as sns
from   torch_geometric.data import Data, DataLoader
import torch_geometric
from   torch_geometric.nn import GCNConv
from   torch_geometric.utils import to_networkx
import torch
import torch_xla
import torch_xla.core.xla_model as xm
import torch.nn as nn
import torch.nn.functional as F
import torch
from   tqdm import tqdm
from   util.get_stock_price import get_open_close_prices
from   util.predict import predict_for_singletext
import warnings

nltk.download('stopwords')
warnings.filterwarnings("ignore", category=FutureWarning)
tqdm.pandas()

# Read the csv file
stocks = pd.read_csv(os.path.join(DATADIR, 'dataset', 'stocks_cleaned.csv'))
stockerbot = pd.read_csv(os.path.join(DATADIR, 'dataset', 'stockerbot-export.csv'), error_bad_lines=False)
stockerbot = stockerbot.drop(columns=['id', 'url', 'verified'])

# Casting the columns acrroding to our use
stockerbot["timestamp_alter"] = pd.to_datetime(stockerbot["timestamp"])
stockerbot["text"] = stockerbot["text"].astype(str)
stockerbot["company_names"] = stockerbot["company_names"].astype("category")
stockerbot["symbols"] = stockerbot["symbols"].astype("category")
stockerbot["source"] = stockerbot["source"].astype("category")


# Create separate date & time columns for easier manipulation
stockerbot['date'] = stockerbot['timestamp_alter'].dt.date
stockerbot['time'] = stockerbot['timestamp_alter'].dt.time

# Handeling the missing values
stockerbot.isnull().any()
stockerbot[stockerbot['company_names'].isnull()]

# Removing links from the tweets
def remove_urls(text):
    url_pattern = r'https?://\S+|www\.\S+'
    return re.sub(url_pattern, '', text)

stockerbot['text'] = stockerbot['text'].progress_apply(remove_urls)
stockerbot.dtypes

# Date wise count of all tweets
stockerbot['date'].value_counts()

# Trim it for testing purpose
stockerbot = stockerbot.head(100)

# Get the sentiment of the tweet
def get_message_sentiment(text):
    return predict_for_singletext(text)[1]

stockerbot['msg_type_logits'] = stockerbot['text'].progress_apply(get_message_sentiment)

# Define a function to calculate open and close prices and apply it row-wise
def calculate_open_close_prices(row):
    open, close = get_open_close_prices(row['symbols'], row['timestamp'])
    return pd.Series({'open_price': open, 'close_price': close})

stockerbot[['open_price', 'close_price']] = stockerbot.progress_apply(calculate_open_close_prices, axis=1)
stockerbot = stockerbot.dropna(subset=['open_price', 'close_price'])
# Reset the index of the DataFrame
stockerbot.reset_index(drop=True, inplace=True)

stockerbot["outcome"] = stockerbot["close_price"] - stockerbot["open_price"]

# Convert msg_type_logits to numpy array
msg_type_logits = np.array(stockerbot['msg_type_logits'].tolist())

# Determine if the source is smart only based on positive and neagtive
stockerbot['is_smart'] = np.where(msg_type_logits[:, 0] > msg_type_logits[:, 1], stockerbot['outcome'] > 0, stockerbot['outcome'] < 0)
stockerbot.head(3)

total_true, total_false = stockerbot['is_smart'].value_counts()
# This is based on the is_smart dataset
threshold = (total_true - total_false) / (total_true + total_false)
print(total_true, total_false, threshold)

# Get all the unique source and target nodes
source_nodes = list(set(stockerbot['source'].unique()))
target_nodes = list(set(stockerbot['symbols'].unique()))

nodes = list(set(stockerbot['source'].unique()) | set(stockerbot['symbols'].unique()))

# Create a dictionary for mapping node to index
node_to_index = {node: index for index, node in enumerate(nodes)}

# Create node_features
node_features = []
for node in nodes:
    source_feature = 1 if node in source_nodes else 0
    target_feature = 1 if node in target_nodes else 0
    node_features.append([source_feature, target_feature])
node_features = torch.tensor(node_features, dtype=torch.float)
node_features

# Create edge_index
edge_index = []
for index, row in stockerbot.iterrows():
    source_index = node_to_index[row['source']]
    target_index = node_to_index[row['symbols']]
    edge_index.append([source_index, target_index])
edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
edge_index

# Create edge_features
edge_features = []
for index, row in stockerbot.iterrows():
    msg_type_logits_features = row['msg_type_logits']
    multiplyer = 1.0 if row['is_smart'] else -1.0
    msg_type_logits_features = np.multiply(msg_type_logits_features, multiplyer)
    msg_type_logits_features = np.insert(msg_type_logits_features, 0, multiplyer)
    edge_features.append(msg_type_logits_features)
edge_features = torch.tensor(edge_features, dtype=torch.float)
edge_features

# Creating a PyTorch Geometric Data object
data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)
data

if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

# Check if TPU is available
#if torch_xla._XLAC._xla_get_devices() != []:
#    device = xm.xla_device()

device

class EdgeFeaturePredictor(nn.Module):
    def __init__(self, num_node_features, num_edge_features):
        super(EdgeFeaturePredictor, self).__init__()
        self.node_encoder = GCNConv(num_node_features, 16)
        self.edge_predictor = nn.Linear(32, num_edge_features)

    def forward(self, data, edge_index):
        x = F.relu(self.node_encoder(data.x, data.edge_index))
        edge_features = torch.cat([x[edge_index[0]], x[edge_index[1]]], dim=1)
        edge_features = self.edge_predictor(edge_features)

        # Apply tanh to the first value and softmax to the last three values
        edge_features = torch.cat([torch.tanh(edge_features[:, :1]), F.softmax(edge_features[:, 1:], dim=1)], dim=1)

        return edge_features

# Instantiate the EdgeFeaturePredictor Model
num_node_features = node_features.size(1)
num_edge_features = edge_features.size(1)
edge_feature_predictor = EdgeFeaturePredictor(num_node_features, num_edge_features).to(device)

# Example training loop
optimizer = torch.optim.Adam(edge_feature_predictor.parameters(), lr=0.01)

for epoch in tqdm(range(1000)):
    edge_feature_predictor.train()
    optimizer.zero_grad()
    predicted_edge_features = edge_feature_predictor(data.to(device), edge_index.to(device))
    loss = nn.MSELoss()(predicted_edge_features, edge_features.to(device))
    loss.backward()
    optimizer.step()

# Making predictions
edge_feature_predictor.eval()
with torch.no_grad():
    predicted_edge_features = edge_feature_predictor(data.to(device), edge_index.to(device))

print("Predicted Edge Features:", predicted_edge_features)

# Node classification based on edge features
node_classification = []
for node in range(node_features.size(0)):
    # Only classify nodes with features [1, 0] or [1, 1]
    if node_features[node].tolist() not in [[1.0, 0.0], [1.0, 1.0]]:
        continue

    # Get the edges for the current node
    node_edges = (edge_index[0] == node) | (edge_index[1] == node)

    # Calculate the average of the first value of the edge features
    avg_edge_feature = predicted_edge_features[node_edges, 0].mean()

    # Classify the node
    node_classification.append((node, avg_edge_feature >= threshold))

node_classification = [(node, torch.tensor(classification).long().to(device)) for node, classification in node_classification]  # Convert to long tensor for use with CrossEntropyLoss
node_classes = ["Non Reliable", "Reliable"]
print("Node Classes:")
count_reliable = 0
count_non_reliable = 0
for node, classification in node_classification:
    predict = node_classes[classification]
    print(node, predict)
    if predict == "Reliable":
        count_reliable += 1
    else:
       count_non_reliable += 1

print(count_reliable, count_non_reliable)

# Convert edge_index to a set for faster lookup
existing_edges = set(zip(edge_index[0].tolist(), edge_index[1].tolist()))

# Initialize lists to hold the new edges
new_source_nodes = []
new_target_nodes = []

# Iterate over all possible edges
for source_name in source_nodes:
    for target_name in target_nodes:
        source = node_to_index[source_name]
        target = node_to_index[target_name]
        # Check if the edge is not in the existing edges
        if (source, target) not in existing_edges:
            new_source_nodes.append(source)
            new_target_nodes.append(target)

# Convert the lists to a tensor
new_edges = torch.tensor([new_source_nodes, new_target_nodes], dtype=torch.long)

# Make predictions for the new edges
edge_feature_predictor.eval()
with torch.no_grad():
    new_predicted_edge_features = edge_feature_predictor(data, new_edges)

# Print the predicted edge features for the new edges
print("Predicted Edge Features for New Edges:")
print(new_predicted_edge_features)
print(len(new_predicted_edge_features))

# Create a new directed graph
dot = Digraph()

# Convert node_classification to a dictionary
node_classification_dict = dict(node_classification)

# Add nodes to the graph
node_colors = []
for node in range(node_features.size(0)):
    if node_features[node].tolist() in [[1, 0], [1, 1]]:
        # Check the node_classification_dict
        node_colors.append('green' if node_classification_dict.get(node, 0) else 'red')
    else:
        node_colors.append('grey')
    dot.node(str(node), color=node_colors[node], style='filled')

# Add old edges to the graph and store edge weights
old_edge_weights = defaultdict(list)
for i in range(edge_index.size(1)):
    node1 = edge_index[0, i].item()
    node2 = edge_index[1, i].item()
    weight = 1 - max(predicted_edge_features[i, 1:].tolist())
    old_edge_weights[(str(node1), str(node2))].append(weight)

# Add new edges to the graph and store edge weights
new_edge_weights = defaultdict(list)
for i in range(new_edges.size(1)):
    node1 = new_edges[0, i].item()
    node2 = new_edges[1, i].item()
    weight = 1 - max(new_predicted_edge_features[i, 1:].tolist())
    new_edge_weights[(str(node1), str(node2))].append(weight)

# Calculate average weights for overlapping old edges and add them to the graph
for edge, weights in old_edge_weights.items():
    avg_weight = round(sum(weights) / len(weights), 2)
    dot.edge(edge[0], edge[1], label=str(avg_weight), color='black')

# Calculate average weights for overlapping new edges and add them to the graph
for edge, weights in new_edge_weights.items():
    avg_weight = round(sum(weights) / len(weights), 2)
    dot.edge(edge[0], edge[1], label=str(avg_weight), color='blue', style='dashed')

# Render the graph
dot.render('graph.png', view=True)

# Create a new directed graph
G = nx.DiGraph()

# Convert node_classification to a dictionary
node_classification_dict = dict(node_classification)

# Add nodes to the graph
node_colors = []
for node in range(node_features.size(0)):
    G.add_node(node)
    if node_features[node].tolist() in [[1, 0], [1, 1]]:
        # Check the node_classification_dict
        node_colors.append('green' if node_classification_dict.get(node, 0) else 'red')
    else:
        node_colors.append('grey')

# Add edges to the graph and store edge weights
edge_weights = defaultdict(list)
for i in range(edge_index.size(1)):
    node1 = edge_index[0, i].item()
    node2 = edge_index[1, i].item()
    weight = 1 - max(predicted_edge_features[i, 1:].tolist())
    G.add_edge(node1, node2, color='black')
    edge_weights[(node1, node2)].append(weight)

# Add new edges to the graph
for i in range(new_edges.size(1)):
    node1 = new_edges[0, i].item()
    node2 = new_edges[1, i].item()
    weight = 1 - max(new_predicted_edge_features[i, 1:].tolist())
    G.add_edge(node1, node2, color='blue')
    edge_weights[(node1, node2)].append(weight)

# Calculate average weights for overlapping edges
for edge in edge_weights:
    edge_weights[edge] = round(sum(edge_weights[edge]) / len(edge_weights[edge]), 2)

# Get the positions of the nodes using the spring layout
pos = nx.spring_layout(G)

# Draw the nodes
nx.draw_networkx_nodes(G, pos, node_color=node_colors)

# Draw the edges
edges = G.edges()
edge_colors = [G[u][v]['color'] for u, v in edges]
edge_styles = ['dashed' if G[u][v]['color'] == 'blue' else 'solid' for u, v in edges]
nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=edge_colors, style=edge_styles, arrows=True)

# Draw the node labels
nx.draw_networkx_labels(G, pos)

# Draw the edge labels
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_weights)

# Show the plot
plt.show()

def finbert_accuracy(stockerbot):
    correct_predictions = 0
    total_predictions = 0

    for index, row in stockerbot.iterrows():
        sentiment = 'Positive' if row['msg_type_logits'][0] > row['msg_type_logits'][1] else 'Negative'
        correct = (sentiment == 'Positive' and row['outcome'] > 0) or (sentiment == 'Negative' and row['outcome'] < 0)
        correct_predictions += correct
        total_predictions += 1

    return correct_predictions / total_predictions

finbert_acc = finbert_accuracy(stockerbot)
print("FinBERT Accuracy: {:.2%}".format(finbert_acc))

def full_model_accuracy(stockerbot, node_classification):
    correct_predictions = 0
    total_predictions = 0

    for node, classification in node_classification:
        for index, row in stockerbot[stockerbot['source'] == nodes[node]].iterrows():
            correct = (classification and row['outcome'] > 0) or (not classification and row['outcome'] < 0)
            correct_predictions += correct
            total_predictions += 1

    return correct_predictions / total_predictions if total_predictions > 0 else 0

full_model_acc = full_model_accuracy(stockerbot, node_classification)
print("Full Model Accuracy: {:.2%}".format(full_model_acc))

print("FinBERT Accuracy: {:.2%}".format(finbert_acc))
print("Full Model Accuracy: {:.2%}".format(full_model_acc))